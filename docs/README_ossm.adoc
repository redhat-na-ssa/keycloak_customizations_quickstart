:scrollbar:
:data-uri:
:toc2:
:linkattrs:


= OSSM / SSO / API Mgmt Integrations

:numbered:

== OSSM Pre-reqs

=== Environment Variables

. Set the following environment variables with values similar to the following:
+
-----
export OCP_DOMAIN=apps$(oc whoami --show-console | awk 'BEGIN{FS="apps"}{print $2}')
export RHSSO_HOST=sso-rhi-idm.$OCP_DOMAIN
export RHSSO_URL=https://$RHSSO_HOST/auth
export RHSSO_MASTER_PASSWD=$(oc get secret credential-rhsso -o json -n rhi-idm | jq -r .data.ADMIN_PASSWORD | base64 -d)
export REALM_ID=user1-ldap
export SSO_CLIENT_ID=ldap-app      # preset in realm deployed by project ansible
export ACCESS_TOKEN_URL="$RHSSO_URL/realms/$REALM_ID/protocol/openid-connect/token"
export INGRESS_HOST=secure-ingress.$OCP_DOMAIN
export SM_CP_NS=admin1-istio-system
-----

=== `istioctl`

Some exercises in this quickstart use the `istioctl` Istio command line tool. +
You can ownload the upstream Istio distribution from link:https://github.com/istio/istio/releases/tag/1.9.9[]. 

Make sure to download the _1.9.9_ version, which corresponds with the upstream Istio version that Red Hat Service Mesh is based on.

Extract the archive to a directory on your $PATH.


=== OSSM Control Plane
You'll need _OpenShift Service Mesh_ installed in your OCP cluster.

For the purpose of this quickstart documentation, the istio control plane is installed in a namespace called:  `admin1-istio-system`.  Notice you previously set an environment variable called `SM_CP_NS` to the name of this namespace.

Verify the version of OSSM that is installed by executing the following at the command line: 

-----
$ istioctl version --remote=true -i $SM_CP_NS


client version: 1.9.9
control plane version: OSSM_2.1.2-4.el8
data plane version: 1.9.9 (4 proxies)
-----

=== `ServiceMeshMemberRoll`

. Register the `user1-services` namespace as a member to be monitored and managed by your service mesh control plane:
+
-----
$ echo "apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - user1-services" | oc apply -n $SM_CP_NS -f -
-----

* NOTE:  The `ServiceMeshMemberRoll` resource exists in the service mesh control plane, which is typically owned by either a cluster-admin or service mesh admin.

. Verify that your `user1-services` namespace now includes `kiali` and `maistra` annotations:
+
-----
$ echo -en "\n\n$(oc get project user1-services -o template --template='{{.metadata.labels}}')\n\n"
-----
+
.Sample Output
-----
map[kiali.io/member-of:admin1-istio-system kubernetes.io/metadata.name:user1-services maistra.io/member-of:admin1-istio-system olm.operatorgroup.uid/df105b19-bc40-4fcc-8fe2-f2c41bb19999:]
-----

. Verify that your `user1-services` namespace now includes namespace-scoped `RoleBinding` resources associated with the Istio-related service accounts from your specific service mesh control plane:
+
-----
$ oc get RoleBinding  -n user1-services -l release=istio
NAME                                       ROLE                                                       AGE
istio-egressgateway-sds                    Role/istio-egressgateway-sds                               13d
istio-ingressgateway-sds                   Role/istio-ingressgateway-sds                              13d
istiod-full-install-admin1-istio-system    ClusterRole/istiod-full-install-admin1-istio-system        13d
istiod-internal-full-install               Role/istiod-internal-full-install                          13d
prometheus-admin1-istio-system             ClusterRole/prometheus-admin1-istio-system                 13d
wasm-cacher-full-install                   ClusterRole/wasm-cacher-full-install-admin1-istio-system   13d
wasm-cacher-registry-viewer-full-install   ClusterRole/registry-viewer                                13d
-----
* The use of a project-scoped `RoleBinding` resource, rather than a cluster-scoped `ClusterRoleBinding` resource, is a key enabler of the _multi-tenant_ capabilities of Red Hat^(R)^ OpenShift^(R)^ Service Mesh.

* OpenShift Service Mesh configures each member project to ensure network access between itself, the control plane, and other member projects.

. Verify that your `$ERDEMO_NS` namespace now also includes a `NetworkPolicy` resource called `istio-mesh`:
+
-----
$ oc get NetworkPolicy istio-mesh-full-install -n $ERDEMO_NS
-----
+
.Sample Output
-----
NAME                      POD-SELECTOR   AGE
istio-mesh-full-install   <none>         59m
-----
* This `NetworkPolicy` resource allows ingress to all pods specific to this namespace from all other registered members of the same OpenShift Service Mesh control plane.


=== Opt In Auto-Injection Annotations

When deploying an application into Red Hat OpenShift Service Mesh, you must opt in to injection of the Envoy _data plane_ for each deployment.
You do so by specifying the `sidecar.istio.io/inject=true` annotation in your deployment.

Opting in ensures that the sidecar injection does not interfere with other OpenShift capabilities (such as S2I builder pods) that likely do not need to be managed by the service mesh.

In this section of the lab, you, as the owner of the Emergency Response Demo application, opt in a selective list of deployments for auto injection of a sidecar.

. Switch to the `$ERDEMO_USER` user:
+
-----
$ oc login -u $ERDEMO_USER
-----
* `$ERDEMO_USER` is the admin of the `$ERDEMO_NS` namespace where your Emergency Response Demo application resides.

. Review the contents of link:https://github.com/gpe-mw-training/ocp_service_mesh_advanced/blob/master/utils/inject_istio_annotation.sh[this script], which iterates through the DeploymentConfig of your Emergency Response Demo application and adds the `sidecar.istio.io/inject: "true"` annotation.
+
****
*Questions*:

* Which DeploymentConfig resources of the Emergency Response Demo application are to be opted into your service mesh?
* Which resources of the Emergency Response Demo application will not be managed by your service mesh?
****

. Execute the shell script that adds Envoy auto-injection annotations to Emergency Response Demo deployments:
+
-----
$ curl https://raw.githubusercontent.com/gpe-mw-training/ocp_service_mesh_advanced/master/utils/inject_istio_annotation.sh \
    -o $HOME/lab/inject_istio_annotation.sh && \
    chmod 775 $HOME/lab/inject_istio_annotation.sh && \
    $HOME/lab/inject_istio_annotation.sh
-----

. After completion of the script, review the list of Emergency Response-related pods:
+
-----
$ oc get pods -l group=erd-services -n $ERDEMO_NS
-----
+
.Sample Output
-----
user50-disaster-simulator-1-p9gfl          2/2     Running   7          9h
user50-incident-priority-service-1-hgmdn   2/2     Running   4          9h
user50-incident-service-1-sz4dk            2/2     Running   3          9h
user50-mission-service-1-jz2r8             2/2     Running   9          9h
user50-process-service-4-cz5sz             2/2     Running   5          7h17m
user50-responder-service-1-qm5gn           2/2     Running   3          7h14m
user50-responder-simulator-1-tdrz2         2/2     Running   6          7h13m
-----
* Note that each of these pods indicates that two containers have started.

. Use a script similar to this to identify a list of container names for each of the pods:
+
-----
$ for POD_NAME in $(oc get pods -n $ERDEMO_NS -l group=erd-services -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
    oc get pod $POD_NAME  -n $ERDEMO_NS -o jsonpath='{.metadata.name}{"    :\t\t"}{.spec.containers[*].name}{"\n"}'
done
-----
+
.Sample Output
-----
[...]
user50-disaster-simulator-1-p9gfl    :          user50-disaster-simulator        istio-proxy
user50-incident-priority-service-1-hgmdn    :   user50-incident-priority-service istio-proxy
user50-incident-service-1-sz4dk    :            user50-incident-service          istio-proxy
user50-mission-service-1-jz2r8    :             user50-mission-service           istio-proxy
user50-process-service-4-cz5sz    :             user50-process-service           istio-proxy
user50-responder-service-1-qm5gn    :           user50-responder-service         istio-proxy
user50-responder-simulator-1-tdrz2    :         user50-responder-simulator       istio-proxy
-----

* Note that each pod now contains an additional `istio-proxy` container colocated with the primary business service container.
* Recall from a previous lab that OpenShift Service Mesh uses a Kubernetes link:https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook[`MutatingAdmissionWebhook`] for automatically injecting the sidecar proxy into user pods.



== Restrict User Access Via Service Mesh 2.0 & RH-SSO





=== Add Services to Mesh

-----
echo "apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  name: frontend
spec:
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: \"true\"" \
  | oc apply -n user1-services -f -
-----

-----
echo "apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  name: backend-oidc
spec:
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: \"true\"" \
  | oc apply -n user1-services -f -
-----


=== Use Istio native mechanisms for JWT-based authorization
The purpose of this quickstart is to demonstrate exposure of a REST API deployed in OSSM where the request must include a valid JWT.

. Delete existing _frontend_ route:
+
-----
$ oc delete route frontend -n user1-services
-----

. Create route that targets istio ingress gateway:
+
-----
$ sed "s|%INGRESS_HOST%|$INGRESS_HOST|" ./ossm/ingress-route.yml |  oc apply -n admin1-istio-system -f -
-----

. Create gateway:
+
-----
$ sed "s|%INGRESS_HOST%|$INGRESS_HOST|" ./ossm/frontend-gw.yml |  oc apply -n user1-services -f -
-----

. Create virtual service:
+
-----
$ sed "s|%INGRESS_HOST%|$INGRESS_HOST|" ./ossm/frontend-vs.yml |  oc apply -n user1-services -f -
-----

. Retrieve token from RH-SSO:
+
-----
$ TKN=$(curl -X POST "$ACCESS_TOKEN_URL" \
            -H "Content-Type: application/x-www-form-urlencoded" \
            -d "username=jbrown" \
            -d "password=password" \
            -d "grant_type=password" \
            -d "client_id=$SSO_CLIENT_ID" \
            -d "scope=openid" \
            | sed 's/.*access_token":"//g' | sed 's/".*//g')
-----

. Invoke frontend service via istio ingress gateway:
+
-----
$ curl -v -H "Authorization: Bearer $TKN"        -X GET https://$INGRESS_HOST/frontend
-----

. Inspect JWT:
+
-----
$ jq -R 'split(".") | .[1] | @base64d | fromjson' <<< $TKN > /tmp/jwt.json

$ cat /tmp/jwt.json
-----

. Create RequestAuthentication and AuthorizationPolicy to only allow requests with valid JWT:
+
-----
$ sed "s|%RHSSO_URL%|$RHSSO_URL|" ./ossm/frontend-request-auth.yml \
    | sed "s|%REALM_ID%|$REALM_ID|" \
    |  oc apply -n user1-services -f -
-----

. NOT NEEDED:  Create external serviceentry to RH-SSO:
+
-----
$ sed "s|%RHSSO_HOST%|$RHSSO_HOST|" ./ossm/keycloak-serviceentry.yml \
    |  oc apply -n user1-services -f -
-----

. Start webapp:
+
-----
$ cd webapp

$ export KC_URL=$RHSSO_URL \
  && export KC_REALM_ID=$REALM_ID \
  && export SERVICE_URL=https://$INGRESS_HOST/frontend \
  && npm start
-----

=== Inject oauth2-proxy in Istio ingress gateway to implement an OIDC workflow

TO-DO


=== Reference

. link:https://cloud.redhat.com/blog/restricting-user-access-via-service-mesh-2.0-and-red-hat-single-sign-on[Restricting User Access Via Service Mesh 2.0 & RH-SSO]
+
Appears need to upgrade to:
+
https://github.com/oauth2-proxy/oauth2-proxy/blob/master/docs/docs/configuration/auth.md#keycloak-oidc-auth-provider
+
Discuss three different approaches:

.. Approach 1: Using Istio native mechanisms for JWT-based authorization
.. Approach 2: Injecting oauth2-proxy container inside the Istio ingress gateway to implement an OIDC workflow
.. Approach 3: Combining JWT-based authorization and OIDC workflow

. link:https://homelab.blog/blog/devops/Istio-OIDC-Config/[Configuring Istio w/ OIDC Authentication]
+
Makes use of "oidc" provider of oauth2-proxy rather than "keycloak" provider.
+
Introduces a Redis database as a workaround for an apparent bug at the time.
+
Uses what seems to be an old container image:  quay.io/pusher/oauth2_proxy:v4.1.0


. link:https://medium.com/@senthilrch/api-authentication-using-istio-ingress-gateway-oauth2-proxy-and-keycloak-a980c996c259[Ingress GW, OAuth2-Proxy and RH-SSO  .... similar to previous link]

. link:https://github.com/RedHatGov/service-mesh-workshop-dashboard/blob/main/workshop/content/lab5.4_authpolicy.md[Lab:  Authorizing and Authenticating Access via Policy]


== Apply API Policies to Ingress Gateway traffic

=== Reference

. https://github.com/3scale-demos/ossm-3scale-wasm.git
. https://developers.redhat.com/articles/2021/12/06/custom-webassembly-extensions-openshift-service-mesh[Satya's writeup]
. https://github.com/3scale/threescale-wasm-auth/[github: threescale-wasm-auth]
. link:https://quay.io/repository/3scale/threescale-wasm-auth?tab=tags&tag=latest[3scale WASM container images]

=== Prereqs

=== Envoy Proxy troubleshooting

. Set the log level of the _wasm_ and _http_ modules of _istio-proxy_ to debug: 
+
-----
$ oc rsh $(oc get pod | grep "^frontend" | awk '{print $1}') curl -X POST http://localhost:15000/logging?wasm=debug \
  && oc rsh $(oc get pod | grep "^frontend" | awk '{print $1}') curl -X POST http://localhost:15000/logging?http=debug
-----

. In a new terminal, tail the log file of the _frontend_ pod's _istio-proxy_ container: 
+
-----
$ oc logs -c istio-proxy -f $(oc get pod | grep "^frontend" | awk '{print $1}')
-----

. Optional:  Advanced envoy debugging:
.. Using the _istioctl_ utility, execute the following:
+
-----
$ istioctl dashboard envoy $(oc get pod | grep "^frontend" | awk '{print $1}').user1-services
-----
.. Navigate to _config_dump_


=== Procedure

. View list of tags for _3scale-auth-wasm_rhel8_ container image: 
+
-----
$ skopeo login registry.redhat.io -u <userId> -p <passwd>

$ skopeo list-tags docker://registry.redhat.io/openshift-service-mesh/3scale-auth-wasm-rhel8

-----

. Set Environment Variables: 
+
-----
# export TENANT_SECRET=adprod-generated-secret  # only applicable for fix in 3scale 2.12
export TENANT_SECRET=adprod-atoken-secret  # appropriate for 3scale 2.11
export TENANT_ADMIN_HOST=$(oc get secret $TENANT_SECRET -o json -n rhi-apimgmt | jq '.data.adminURL' -r | base64 -d | sed 's/https:\/\///' )
export TENANT_ACCESS_TOKEN=$(oc get secret $TENANT_SECRET -o json -n rhi-apimgmt | jq '.data.token' -r | base64 -d)
export API_MGMT_BACKEND_HOST=$( oc get route backend -n rhi-apimgmt --template='{{ .spec.host }}' )
export API_SERVICE_ID=$(oc get product adprod-quarkus-product -n rhi-apimgmt -o json | jq .status.productId)
export API_SERVICE_TOKEN=$( curl https://$TENANT_ACCESS_TOKEN@$TENANT_ADMIN_HOST/admin/api/services/$API_SERVICE_ID/proxy/configs/production/latest.json | jq -r '.proxy_config.content.backend_authentication_value'  )
export API_APP_ID=CHANGEME
-----

. Create a ServiceEntry allowing API gateway WASM to invoke 3scale Admin Provider URL: 
+
-----
$ sed "s|%TENANT_ADMIN_HOST%|$TENANT_ADMIN_HOST|" ./ossm/3scale-wasm/serviceentry-adminURL.yml |  oc apply -n user1-services -f -
-----

. Create a ServiceEntry allowing API gateway WASM to invoke 3scale backend URL: 
+
-----
$ sed "s|%API_MGMT_BACKEND_HOST%|$API_MGMT_BACKEND_HOST|" ./ossm/3scale-wasm/serviceentry-backendURL.yml |  oc apply -n user1-services -f -
-----
+
NOTE:  Without this ServiceEntry, an error will appear in the _envoy-proxy_ logs similar to the following: 
+
-----
error	envoy wasm	wasm log:    233614343 (1/http): on_http_request_headers: could not dispatch HTTP call to outbound|443||backend-3scale.apps.den.ratwater.xyz: did you create the cluster to do so? - "failed to dispatch HTTP (https) call to cluster outbound|443||backend-3scale.apps.den.ratwater.xyz with authority backend-3scale.apps.den.ratwater.xyz: BadArgument"

-----



. Create a new ServiceMeshExtension: 
+
-----
$ . ossm/3scale-wasm/servicemeshextension.yml | oc apply -n user1-services -f -
-----
+
NOTE: In the logs of the _istio-proxy_, you might see warnings such as the following: 
+
-----
2022-04-27T21:39:08.178317Z	warning	envoy wasm	wasm log threescale-wasm-adprod_root :  (root/1)  331943116: on_vm_start: empty VM config
2022-04-27T21:39:08.206972Z	warning	envoy wasm	wasm log threescale-wasm-adprod_root :  (root/1) 3991748476: on_vm_start: empty VM config
2022-04-27T21:39:08.207477Z	warning	envoy wasm	wasm log threescale-wasm-adprod_root :  (root/1) 1626258163: on_vm_start: empty VM config
2022-04-27T21:39:08.207534Z	warning	envoy wasm	wasm log threescale-wasm-adprod_root :  (root/1)  307150597: on_vm_start: empty VM config

-----
+
These warnings are benign and can be ignored.
What you should not see are any errors in the log.



. Retrieve token from RH-SSO:
+
-----
$ TKN=$(curl -X POST "$ACCESS_TOKEN_URL" \
            -H "Content-Type: application/x-www-form-urlencoded" \
            -d "username=jbrown" \
            -d "password=password" \
            -d "grant_type=password" \
            -d "client_id=$API_APP_ID" \
            -d "scope=openid" \
            | sed 's/.*access_token":"//g' | sed 's/".*//g')
-----

. Invoke frontend service via istio ingress gateway:
+
-----
$ curl -v -H "Authorization: Bearer $TKN"        -X GET https://$INGRESS_HOST/frontend
-----

. Existing error when servicemeshextension configured to use https to backend:

.. logs: 
+
-----
}
2022-04-28T02:03:48.058188Z	debug	envoy wasm	wasm log:    1625927304 (1/http): matched pattern in /frontend
2022-04-28T02:03:48.058378Z	info	envoy wasm	wasm log: calling out outbound|443||backend-3scale.apps.den.ratwater.xyz (using https scheme) with headers -> [(":authority", "backend-3scale.apps.den.ratwater.xyz"), (":scheme", "https"), (":method", "GET"), (":path", "/transactions/authrep.xml?service_id=13&service_token=ea06d62d7cb0659140d0b31384f789a9464934feedf478fe8030933529e331f4&app_id=84493d52&usage[hits]=1"), ("3scale-options", "no_body=1"), ("User-Agent", "threescalers/0.8.0")] <- and body -> "(nothing)" <-
2022-04-28T02:03:48.059140Z	info	envoy wasm	wasm log:     1625927304 (1/http): on_http_request_headers: call token is 4
2022-04-28T02:03:48.060876Z	info	envoy wasm	wasm log:     1625927304 (1/http): http_ctx: on_http_call_response: token id is 4
2022-04-28T02:03:48.060927Z	info	envoy wasm	wasm log:     1625927304 (1/http): on_http_call_response: forbidden 4
2022-04-28T02:03:48.060953Z	debug	envoy wasm	wasm log:    1625927304 (1/http): 403 sent

-----

.. Jiras

... link:https://github.com/3scale/threescale-wasm-auth/issues/79[79].
... link:https://issues.redhat.com/browse/THREESCALE-7919[THREESCALE-7919]
